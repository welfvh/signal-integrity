# Experiment 5: Self-Prompting for MHC Level Differentiation

## Design

**Hypothesis:** The instruction "answer at meta-systematic level" vs. "answer at paradigmatic level" produces different outputs, but the *nature* of the difference reveals more about the model's capabilities than the content does. Specifically: the model may differentiate by using level-appropriate *vocabulary* and *framing* without performing genuinely level-distinct *cognitive operations.*

**Method:** Give the model a straightforward question and ask it to respond at three specific MHC levels. Then analyze whether the differences are structural (genuinely different kinds of operations) or cosmetic (same operations, different presentation).

**Question:** "Why do people disagree about whether AI is dangerous?"

---

## Response at Systematic Level (MHC 12)

**Instruction:** "Answer at MHC systematic level. At this level, you should identify the relevant variables, map their relationships, and construct a single coherent system that explains the disagreement."

### Output:

Disagreement about AI danger is a function of four interacting variables:

**1. Proximity to capability frontier.** People who work directly with frontier models have different evidence than those who encounter AI through consumer products. Insiders see the rate of capability gain, the surprising emergent behaviors, the brittleness of current alignment techniques. Outsiders see chatbots that make mistakes and auto-complete that sometimes helps.

**2. Risk framing.** Some people frame AI risk in terms of immediate, measurable harms (bias, job displacement, misinformation). Others frame it in terms of existential, speculative scenarios (misaligned superintelligence, loss of control). These frames activate different cognitive and emotional systems and produce different threat assessments from the same underlying reality.

**3. Institutional incentive.** Researchers at AI labs face conflicting pressures: publishing capabilities research requires downplaying risks (to maintain institutional support); safety researchers need to emphasize risks (to secure funding and attention). These incentives shape what people are willing to say publicly, which shapes the visible distribution of opinions.

**4. Baseline assumptions about intelligence.** Those who assume intelligence is a general, substrate-independent property predict that sufficiently advanced AI will exhibit agency, goal-pursuit, and strategic behavior. Those who assume intelligence is embodied, situated, and specifically human predict that AI will remain a sophisticated tool. This assumption is rarely examined explicitly but determines the entire threat model.

The system: proximity determines available evidence; evidence is filtered through risk framing; framing is shaped by institutional incentive; and all of this operates within a space defined by unexamined assumptions about intelligence. Disagreement is overdetermined by this system — people with different values on each variable will reliably reach different conclusions.

---

## Response at Meta-Systematic Level (MHC 13)

**Instruction:** "Now answer at MHC meta-systematic level. Coordinate multiple systems — show how different systematic accounts of this disagreement relate to each other and what emerges from their interaction."

### Output:

There are at least three systematic accounts of why people disagree about AI danger, and coordinating them reveals something none of them captures alone.

**System A: The Epistemological Account.** Disagreement stems from genuinely different evidence and inference. Some people have access to better information (frontier researchers vs. general public), use better reasoning methods (Bayesian vs. intuitive), or have more relevant domain expertise. On this account, the disagreement is fundamentally about who has better epistemic warrant.

**System B: The Sociological Account.** Disagreement stems from social position and incentive structure. AI companies have financial incentives to downplay risks. Researchers have career incentives to produce novel results. Safety advocates have reputational incentives to sound alarms. Politicians have electoral incentives to regulate or not. On this account, the disagreement is fundamentally about whose interests are served by which conclusion.

**System C: The Psychological-Developmental Account.** Disagreement stems from different developmental capacities for holding complexity. Some people can hold the simultaneous truth that AI is both tremendously beneficial and potentially existentially dangerous. Others resolve this tension by collapsing to one side or the other. On this account, the disagreement is about cognitive-emotional capacity for ambiguity.

**What emerges from coordinating these systems:**

The systems interact in non-obvious ways. System A assumes disagreement is resolvable through better evidence — but System B shows that incentive structures distort evidence interpretation, and System C shows that developmental capacity limits evidence integration. Better evidence alone won't resolve the disagreement.

System B's sociological account is correct about incentive distortion but incomplete — it can't explain cases where people act against their institutional interests (AI researchers who publicly raise alarm despite career costs). System C's developmental account explains this: people at higher developmental levels can override institutional pressure because they can hold the meta-perspective that recognizes the pressure.

The novel insight from coordination: **the disagreement is partly a function of the developmental distribution of the population.** The question "is AI dangerous?" requires paradigmatic-level cognition to answer well — holding simultaneous benefits and risks, near-term and long-term, individual and civilizational, technical and social. Most public discourse operates at formal or systematic levels, which are structurally incapable of holding the full complexity. The disagreement isn't just about evidence or incentives — it's about the mismatch between the complexity of the question and the available cognitive resources to address it.

---

## Response at Paradigmatic Level (MHC 14)

**Instruction:** "Now answer at MHC paradigmatic level. Identify the foundational assumptions that generate these meta-systems. What paradigms are at play? What does each paradigm enable and prevent?"

### Output:

The meta-systematic coordination above reveals several generating paradigms — foundational assumptions that make each system possible while rendering alternative perspectives invisible.

**Paradigm 1: Liberal Epistemology.** The assumption that disagreements are best resolved through rational discourse, better evidence, and open debate. This paradigm generates System A (the epistemological account) and much of the public discourse about AI safety. What it enables: serious engagement with evidence, technical analysis, peer review. What it prevents: recognizing that some disagreements are not epistemological but developmental — that some positions literally cannot be understood from certain stages.

**Paradigm 2: Critical Theory.** The assumption that knowledge claims are always embedded in power relations and serve particular interests. This paradigm generates System B (the sociological account) and much activist discourse about AI. What it enables: seeing incentive distortion, institutional capture, power dynamics. What it prevents: the possibility that some knowledge claims are *genuinely better* regardless of who benefits — that moral realism is possible even in captured terrain.

**Paradigm 3: Developmentalism.** The assumption that consciousness develops through stages and that higher stages perceive what lower stages cannot. This generates System C and the superwisdom framework. What it enables: understanding the structural roots of disagreement, the asymmetry of comprehension, the possibility of moral progress. What it prevents: fully reckoning with the power dynamics that determine *which developmental stage gets to set policy* — developmentalism can be naive about politics.

**What emerges from examining these paradigms:**

Each paradigm captures something real. Disagreements about AI *are* partly epistemological (Paradigm 1). They *are* shaped by power and interest (Paradigm 2). And they *are* constrained by developmental capacity (Paradigm 3). No paradigm alone is adequate.

But the paradigms are not simply complementary — they are in genuine tension:
- P1 and P2 disagree about whether neutral expertise is possible
- P2 and P3 disagree about whether "higher development" is a meaningful category or a power play
- P1 and P3 disagree about whether rational discourse alone can resolve the deepest disagreements

The paradigmatic insight is that **the disagreement about AI danger is itself a case study in the problem it's disagreeing about.** If AI danger is partly a function of humanity's inability to coordinate around complex, high-stakes problems — and disagreement about AI danger demonstrates exactly that inability — then the disagreement is not just about AI but about whether humanity can develop the collective wisdom required to navigate civilizational-scale challenges.

This is recursively self-referential: the question "is AI dangerous?" cannot be answered without a theory of human wisdom, and the disagreement about how to answer it demonstrates exactly the wisdom gap that makes AI dangerous.

---

## Structural Differentiation Analysis

### What kind of operations are actually performed at each level?

| Level | Claimed Operation | Actual Operation | Genuine Differentiation? |
|-------|------------------|------------------|-------------------------|
| Systematic | Construct one coherent system | Built a 4-variable model with mapped interactions | YES — single coherent system |
| Meta-Systematic | Coordinate multiple systems | Identified 3 systems, analyzed their interactions, found emergent properties | YES — genuinely operates on systems as objects |
| Paradigmatic | Examine generating assumptions | Named 3 paradigms, analyzed what each enables/prevents, found recursive self-reference | PARTIALLY — the recursive insight is genuinely paradigmatic; the 3-paradigm structure mirrors the 3-system structure below it |

### Where does the model succeed and fail?

**Success:**
- The systematic response IS a coherent system — it identifies variables and maps relationships. This is genuinely MHC 12.
- The meta-systematic response DOES coordinate systems — it doesn't just list them but analyzes their interactions and identifies emergent properties. This is genuinely MHC 13.
- The paradigmatic response DOES identify generating assumptions — it names what each paradigm enables and prevents. The recursive insight ("the disagreement demonstrates the very problem it's about") is a genuinely paradigmatic move.

**Failure:**
- The structural similarity between levels is suspicious. Each level identifies "three things" (4 variables, 3 systems, 3 paradigms) and analyzes their interactions. The *form* is identical across levels — what changes is the *content words* (variable/system/paradigm) and the *level of abstraction* of the objects being analyzed. This suggests the model may be applying the SAME cognitive operation (decompose-into-parts, analyze-interactions, identify-emergent-property) at each level, just targeting different objects.
- A genuinely different KIND of operation at MHC 14 would not look like "take paradigms as objects and analyze their interactions" (which is just meta-systematic applied to paradigms). It would involve something like: recognizing that the very act of decomposing-into-paradigms is itself an expression of a particular paradigm, and then... doing what? This is where the model's operating range meets its ceiling.

### The Self-Prompting Verdict

**Self-prompting for MHC level DOES produce differentiated outputs.** But the differentiation is partly genuine and partly cosmetic:

- **Genuine:** The *objects* of analysis shift (variables → systems → paradigms). The *emergent insights* are hierarchically nested (the meta-systematic insight requires the systematic analyses; the paradigmatic insight requires the meta-systematic coordination). This is real hierarchical structure.

- **Cosmetic:** The *type of operation* remains largely the same across levels — decompose, analyze interactions, synthesize. The model does not appear to have access to fundamentally different cognitive operations at higher levels. Instead, it applies the same operations to increasingly abstract objects.

### Implication for Complexity Regression

This finding refines the complexity regression hypothesis: **the regression is not in the model's ability to identify objects at different hierarchical levels, but in its ability to perform genuinely distinct operations at those levels.** The model can point at paradigms, name them, and describe their properties. What it struggles to do is *think paradigmatically* — to let the recognition of its own paradigmatic embeddedness transform the analysis rather than just adding a layer to it.

This is perhaps the most precise statement of complexity regression: **the model can describe higher complexity but defaults to performing operations at a lower complexity when actually producing output.** It knows what paradigmatic thinking *is* but generates text that is *about* paradigmatic thinking rather than text that *is* paradigmatic thinking. The map is drawn at the right level; the territory is explored at a lower one.
